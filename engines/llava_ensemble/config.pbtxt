name: "llava_ensemble"
platform: "ensemble"
max_batch_size: 16 # Max batch size for the ensemble

input [
  {
    name: "INPUT_IMAGE" # Input for the ensemble (raw image)
    data_type: TYPE_UINT8 # Or FLOAT32 if you preprocess externally
    dims: [ 224, 224, 3 ] # Example: H, W, C
  },
  {
    name: "INPUT_TEXT" # Input for the ensemble (text prompt)
    data_type: TYPE_STRING # For raw text input
    dims: [ 1 ]
  }
]

output [
  {
    name: "OUTPUT_TEXT" # Final output from the ensemble (generated text)
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]

ensemble_scheduling {
  step [
    {
      model_name: "preprocessing_model" # Optional: Python backend for image preprocessing and tokenization
      model_version_policy { latest { num_versions: 1 }}
      input_map {
        key: "INPUT_IMAGE"
        value: "INPUT_IMAGE"
      }
      output_map {
        key: "pixel_values" # Output of preprocessing model for vision encoder
        value: "pixel_values"
      }
      output_map {
        key: "input_ids" # Output of preprocessing model for LLM (tokenized text)
        value: "input_ids_llm" # Give it a different name to distinguish from vision output
      }
      output_map {
        key: "attention_mask"
        value: "attention_mask_llm"
      }
      output_map {
        key: "position_ids"
        value: "position_ids_llm"
      }
    },
    {
      model_name: "vision_encoder_model"
      model_version_policy { latest { num_versions: 1 }}
      input_map {
        key: "pixel_values"
        value: "pixel_values" # From preprocessing step
      }
      output_map {
        key: "last_hidden_state"
        value: "vision_features" # Output of vision encoder
      }
    },
    {
      model_name: "llm_model"
      model_version_policy { latest { num_versions: 1 }}
      input_map {
        key: "input_ids"
        value: "input_ids_llm" # From preprocessing step
      }
      input_map {
        key: "attention_mask"
        value: "attention_mask_llm" # From preprocessing step
      }
      input_map {
        key: "position_ids"
        value: "position_ids_llm" # From preprocessing step
      }
      input_map {
        key: "image_input" # Input to LLM from vision encoder
        value: "vision_features" # From vision encoder step
      }
      output_map {
        key: "output_ids"
        value: "llm_output_ids" # Raw token IDs from LLM
      }
    },
    {
      model_name: "postprocessing_model" # Optional: Python backend for de-tokenization
      model_version_policy { latest { num_versions: 1 }}
      input_map {
        key: "llm_output_ids"
        value: "llm_output_ids"
      }
      output_map {
        key: "OUTPUT_TEXT"
        value: "OUTPUT_TEXT"
      }
    }
  ]
}